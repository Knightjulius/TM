{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = \"cadecv2\"\n",
    "original_path = os.path.join(base_path, \"original\")\n",
    "text_path = os.path.join(base_path, \"text\")\n",
    "output_path = os.path.join(base_path, \"train.txt\")\n",
    "\n",
    "# Function to parse annotations with semicolon-sliced offsets\n",
    "def parse_annotations(file_path):\n",
    "    entities = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"T\"):  # Entity line\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                tag_number, entity_info, text = parts\n",
    "                entity_type, *offsets = entity_info.split(\" \")\n",
    "                offset_ranges = \" \".join(offsets).split(\";\")  # Handle semicolon-separated offsets\n",
    "                for offset_range in offset_ranges:\n",
    "                    start_offset, end_offset = map(int, offset_range.split(\" \"))\n",
    "                    entities.append((start_offset, end_offset, entity_type))\n",
    "    return entities\n",
    "\n",
    "# Function to create IOB labeling\n",
    "def create_iob_labels(text, entities):\n",
    "    labels = [\"O\"] * len(text)  # Initialize all tokens with \"O\"\n",
    "    for start, end, entity_type in entities:\n",
    "        labels[start] = f\"B-{entity_type}\"\n",
    "        for i in range(start + 1, end):\n",
    "            labels[i] = f\"I-{entity_type}\"\n",
    "    return labels\n",
    "\n",
    "# Function to tokenize text and align labels\n",
    "def tokenize_and_label(text, labels):\n",
    "    tokens = text.split()\n",
    "    token_labels = []\n",
    "    text_index = 0\n",
    "    for token in tokens:\n",
    "        token_length = len(token)\n",
    "        if any(char.isalnum() for char in token):  # Skip punctuation\n",
    "            token_label = labels[text_index : text_index + token_length]\n",
    "            label = token_label[0] if token_label else \"O\"\n",
    "            token_labels.append((token, label))\n",
    "        else:\n",
    "            token_labels.append((token, \"O\"))\n",
    "        text_index += token_length + 1  # Move index past the token and space\n",
    "    return token_labels\n",
    "\n",
    "# Process files\n",
    "output_lines = []\n",
    "for text_file in os.listdir(text_path):\n",
    "    text_file_path = os.path.join(text_path, text_file)\n",
    "    annotation_file_path = os.path.join(original_path, text_file.replace(\".txt\", \".ann\"))\n",
    "    \n",
    "    if os.path.exists(annotation_file_path):\n",
    "        # Read text\n",
    "        with open(text_file_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Parse annotations and create labels\n",
    "        entities = parse_annotations(annotation_file_path)\n",
    "        labels = create_iob_labels(text, entities)\n",
    "        token_labels = tokenize_and_label(text, labels)\n",
    "        \n",
    "        # Write to output\n",
    "        for token, label in token_labels:\n",
    "            output_lines.append(f\"{token}\\t{label}\")\n",
    "            if token.endswith(\".\"):  # Add a blank line after sentences\n",
    "                output_lines.append(\"\\n\")\n",
    "\n",
    "# Write the output to train.txt\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of special characters to remove\n",
    "special_characters = [\".\", \",\"]\n",
    "\n",
    "# Cleaning process\n",
    "with open(\"cadecv2/train.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    # Remove special characters\n",
    "    for char in special_characters:\n",
    "        line = line.replace(char, \"\")\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "# Writing the cleaned data to a new file\n",
    "with open(\"train2.txt\", \"w\") as file:\n",
    "    file.writelines(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the file\n",
    "with open(\"train2.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:  # Skip empty lines\n",
    "        processed_lines.append(\"\\n\")\n",
    "        continue\n",
    "\n",
    "    if \"\\t\" in line:  # Process only lines with a tab (word-label pairs)\n",
    "        word, label = line.split(\"\\t\")\n",
    "        if \"'\" in word:  # Check if the word contains an apostrophe\n",
    "            base, suffix = word.split(\"'\", 1)  # Split the word at the apostrophe\n",
    "            processed_lines.append(f\"{base}\\t{label}\\n\")  # Add the base part\n",
    "            processed_lines.append(f\"'{suffix}\\t{label}\\n\")  # Add the suffix with the same label\n",
    "        else:\n",
    "            processed_lines.append(line + \"\\n\")  # Add the original line\n",
    "    else:\n",
    "        processed_lines.append(line + \"\\n\")  # Add lines without tabs as is\n",
    "\n",
    "# Writing the processed data to a new file\n",
    "with open(\"train3.txt\", \"w\") as file:\n",
    "    file.writelines(processed_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
