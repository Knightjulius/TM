{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = \"cadecv2\"\n",
    "original_path = os.path.join(base_path, \"original\")\n",
    "text_path = os.path.join(base_path, \"text\")\n",
    "output_path = os.path.join(base_path, \"train.txt\")\n",
    "\n",
    "# Function to parse annotations with semicolon-sliced offsets\n",
    "def parse_annotations(file_path):\n",
    "    entities = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"T\"):  # Entity line\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                tag_number, entity_info, text = parts\n",
    "                entity_type, *offsets = entity_info.split(\" \")\n",
    "                offset_ranges = \" \".join(offsets).split(\";\")  # Handle semicolon-separated offsets\n",
    "                for offset_range in offset_ranges:\n",
    "                    start_offset, end_offset = map(int, offset_range.split(\" \"))\n",
    "                    entities.append((start_offset, end_offset, entity_type))\n",
    "    return entities\n",
    "\n",
    "# Function to create IOB labeling\n",
    "def create_iob_labels(text, entities):\n",
    "    labels = [\"O\"] * len(text)  # Initialize all tokens with \"O\"\n",
    "    for start, end, entity_type in entities:\n",
    "        labels[start] = f\"B-{entity_type}\"\n",
    "        for i in range(start + 1, end):\n",
    "            labels[i] = f\"I-{entity_type}\"\n",
    "    return labels\n",
    "\n",
    "# Function to tokenize text and align labels\n",
    "def tokenize_and_label(text, labels):\n",
    "    tokens = text.split()\n",
    "    token_labels = []\n",
    "    text_index = 0\n",
    "    for token in tokens:\n",
    "        token_length = len(token)\n",
    "        if any(char.isalnum() for char in token):  # Skip punctuation\n",
    "            token_label = labels[text_index : text_index + token_length]\n",
    "            label = token_label[0] if token_label else \"O\"\n",
    "            token_labels.append((token, label))\n",
    "        else:\n",
    "            token_labels.append((token, \"O\"))\n",
    "        text_index += token_length + 1  # Move index past the token and space\n",
    "    return token_labels\n",
    "\n",
    "# Process files\n",
    "output_lines = []\n",
    "for text_file in os.listdir(text_path):\n",
    "    text_file_path = os.path.join(text_path, text_file)\n",
    "    annotation_file_path = os.path.join(original_path, text_file.replace(\".txt\", \".ann\"))\n",
    "    \n",
    "    if os.path.exists(annotation_file_path):\n",
    "        # Read text\n",
    "        with open(text_file_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Parse annotations and create labels\n",
    "        entities = parse_annotations(annotation_file_path)\n",
    "        labels = create_iob_labels(text, entities)\n",
    "        token_labels = tokenize_and_label(text, labels)\n",
    "        \n",
    "        # Write to output\n",
    "        for token, label in token_labels:\n",
    "            output_lines.append(f\"{token}\\t{label}\")\n",
    "            if token.endswith(\".\"):  # Add a blank line after sentences\n",
    "                output_lines.append(\"\\n\")\n",
    "\n",
    "# Write the output to train.txt\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of special characters to remove\n",
    "special_characters = [\".\", \",\"]\n",
    "\n",
    "# Cleaning process\n",
    "with open(\"cadecv2/train.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    # Remove special characters\n",
    "    for char in special_characters:\n",
    "        line = line.replace(char, \"\")\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "# Writing the cleaned data to a new file\n",
    "with open(\"train2.txt\", \"w\") as file:\n",
    "    file.writelines(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the file\n",
    "with open(\"train2.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:  # Skip empty lines\n",
    "        processed_lines.append(\"\\n\")\n",
    "        continue\n",
    "\n",
    "    if \"\\t\" in line:  # Process only lines with a tab (word-label pairs)\n",
    "        word, label = line.split(\"\\t\")\n",
    "        if \"'\" in word:  # Check if the word contains an apostrophe\n",
    "            base, suffix = word.split(\"'\", 1)  # Split the word at the apostrophe\n",
    "            processed_lines.append(f\"{base}\\t{label}\\n\")  # Add the base part\n",
    "            processed_lines.append(f\"'{suffix}\\t{label}\\n\")  # Add the suffix with the same label\n",
    "        else:\n",
    "            processed_lines.append(line + \"\\n\")  # Add the original line\n",
    "    else:\n",
    "        processed_lines.append(line + \"\\n\")  # Add lines without tabs as is\n",
    "\n",
    "# Writing the processed data to a new file\n",
    "with open(\"train3.txt\", \"w\") as file:\n",
    "    file.writelines(processed_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      "O: 85867\n",
      "B-ADR: 6469\n",
      "I-ADR: 8702\n",
      "B-Drug: 1761\n",
      "B-Disease: 288\n",
      "B-Symptom: 285\n",
      "I-Symptom: 266\n",
      "I-Disease: 171\n",
      "I-Drug: 176\n",
      "B-Finding: 450\n",
      "I-Finding: 392\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define the input file path\n",
    "input_file_path = \"train3.txt\"  # Replace with your file path\n",
    "\n",
    "# Initialize a counter for labels\n",
    "label_counts = Counter()\n",
    "\n",
    "# Processing the file to count labels\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if \"\\t\" in line:  # Process only lines with a tab (word-label pairs)\n",
    "            _, label = line.split(\"\\t\")\n",
    "            label_counts[label] += 1\n",
    "\n",
    "# Display the counts for each label\n",
    "print(\"Label counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"train3.txt\"  # Replace with your file path\n",
    "train_file_path = \"trainFinal.txt\"\n",
    "test_file_path = \"test.txt\"\n",
    "validation_file_path = \"validation.txt\"\n",
    "\n",
    "# Percentages for splitting\n",
    "test_split = 0.10\n",
    "validation_split = 0.10\n",
    "\n",
    "# Read the input file\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Separate sentences by blank lines\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for line in lines:\n",
    "    if line.strip():  # If the line is not empty\n",
    "        current_sentence.append(line)\n",
    "    else:  # If a blank line is encountered, store the sentence\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "\n",
    "# Add the last sentence if file doesn't end with a blank line\n",
    "if current_sentence:\n",
    "    sentences.append(current_sentence)\n",
    "\n",
    "# Shuffle sentences\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Split sentences into train, test, and validation sets\n",
    "test_size = int(len(sentences) * test_split)\n",
    "validation_size = int(len(sentences) * validation_split)\n",
    "\n",
    "test_sentences = sentences[:test_size]\n",
    "validation_sentences = sentences[test_size:test_size + validation_size]\n",
    "train_sentences = sentences[test_size + validation_size:]\n",
    "\n",
    "# Function to write sentences to a file\n",
    "def write_sentences_to_file(sentences, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for sentence in sentences:\n",
    "            for line in sentence:\n",
    "                file.write(line)\n",
    "            file.write(\"\\n\")  # Add a blank line between sentences\n",
    "\n",
    "# Write the splits to respective files\n",
    "write_sentences_to_file(train_sentences, train_file_path)\n",
    "write_sentences_to_file(test_sentences, test_file_path)\n",
    "write_sentences_to_file(validation_sentences, validation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 5859/5859 [00:00<00:00, 109086.35 examples/s]\n",
      "Casting the dataset: 100%|██████████| 732/732 [00:00<00:00, 20763.46 examples/s]\n",
      "Casting the dataset: 100%|██████████| 730/730 [00:00<00:00, 308529.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 5859\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 732\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 730\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, Sequence, ClassLabel\n",
    "\n",
    "# Define the new labels\n",
    "label_names = [\n",
    "    \"O\", \"B-ADR\", \"I-ADR\", \"B-Drug\", \"I-Drug\",\n",
    "    \"B-Disease\", \"I-Disease\", \"B-Symptom\", \"I-Symptom\",\n",
    "    \"B-Finding\", \"I-Finding\"\n",
    "]\n",
    "\n",
    "# Create a mapping from label to integer ID\n",
    "label_mapping = {label: idx for idx, label in enumerate(label_names)}\n",
    "\n",
    "# Function to map labels to integers\n",
    "def label_to_id(label):\n",
    "    return label_mapping.get(label, -100)  # Return -100 for unknown labels\n",
    "\n",
    "# Function to read the BIO file\n",
    "def read_bio_file(filepath):\n",
    "    sentences = []\n",
    "    current_sentence = {\"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            orgline = line\n",
    "            line = line.strip()\n",
    "            if line == \"\":  # Sentence boundary\n",
    "                if current_sentence[\"tokens\"]:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = {\"tokens\": [], \"ner_tags\": []}\n",
    "            else:\n",
    "                # Split the line into token and label\n",
    "                parts = line.split()\n",
    "                if len(parts) == 2:  # Only process lines with exactly two parts\n",
    "                    token, label = parts\n",
    "                    current_sentence[\"tokens\"].append(token)\n",
    "                    current_sentence[\"ner_tags\"].append(label_to_id(label))\n",
    "                else:\n",
    "                    # TODO check how to add these\n",
    "                    #print(f\"Skipping malformed line: {orgline}\")\n",
    "                    continue\n",
    "\n",
    "        # Add the last sentence if the file doesn't end with a blank line\n",
    "        if current_sentence[\"tokens\"]:\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Read the training dataset\n",
    "train_data = read_bio_file(\"trainFinal.txt\")\n",
    "val_data = read_bio_file(\"validation.txt\")\n",
    "test_data = read_bio_file(\"test.txt\")\n",
    "\n",
    "# Load data into the HuggingFace dataset structure\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\n",
    "        \"tokens\": [d[\"tokens\"] for d in train_data],\n",
    "        \"ner_tags\": [d[\"ner_tags\"] for d in train_data]\n",
    "    }),\n",
    "    \"test\": Dataset.from_dict({\n",
    "        \"tokens\": [d[\"tokens\"] for d in test_data],\n",
    "        \"ner_tags\": [d[\"ner_tags\"] for d in test_data]\n",
    "    }),\n",
    "    \"validation\": Dataset.from_dict({\n",
    "        \"tokens\": [d[\"tokens\"] for d in val_data],\n",
    "        \"ner_tags\": [d[\"ner_tags\"] for d in val_data]\n",
    "    })\n",
    "})\n",
    "\n",
    "# Define the ClassLabel feature for NER tags\n",
    "ner_feature = ClassLabel(names=label_names)\n",
    "\n",
    "# Cast the ner_tags column to use the ClassLabel feature\n",
    "dataset = dataset.cast_column(\"ner_tags\", Sequence(ner_feature))\n",
    "\n",
    "# Display the dataset structure\n",
    "print(dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
