{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emmav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\emmav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_path = \"cadecv2\"\n",
    "original_path = os.path.join(base_path, \"original\")\n",
    "text_path = os.path.join(base_path, \"text\")\n",
    "output_path = os.path.join(base_path, \"train.txt\")\n",
    "\n",
    "# Function to parse annotations with semicolon-sliced offsets\n",
    "def parse_annotations(file_path):\n",
    "    entities = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"T\"):  # Entity line\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                tag_number, entity_info, text = parts\n",
    "                entity_type, *offsets = entity_info.split(\" \")\n",
    "                offset_ranges = \" \".join(offsets).split(\";\")  # Handle semicolon-separated offsets\n",
    "                for offset_range in offset_ranges:\n",
    "                    start_offset, end_offset = map(int, offset_range.split(\" \"))\n",
    "                    entities.append((start_offset, end_offset, entity_type))\n",
    "    return entities\n",
    "\n",
    "# Function to create IOB labeling\n",
    "def create_iob_labels(text, entities):\n",
    "    labels = [\"O\"] * len(text)  # Initialize all tokens with \"O\"\n",
    "    for start, end, entity_type in entities:\n",
    "        labels[start] = f\"B-{entity_type}\"\n",
    "        for i in range(start + 1, end):\n",
    "            labels[i] = f\"I-{entity_type}\"\n",
    "    return labels\n",
    "\n",
    "# Function to tokenize text and align labels\n",
    "def tokenize_and_label(text, labels):\n",
    "    tokens = text.split()\n",
    "    token_labels = []\n",
    "    text_index = 0\n",
    "    for token in tokens:\n",
    "        token_length = len(token)\n",
    "        if any(char.isalnum() for char in token):  # Skip punctuation\n",
    "            token_label = labels[text_index : text_index + token_length]\n",
    "            label = token_label[0] if token_label else \"O\"\n",
    "            token_labels.append((token, label))\n",
    "        else:\n",
    "            token_labels.append((token, \"O\"))\n",
    "        text_index += token_length + 1  # Move index past the token and space\n",
    "    return token_labels\n",
    "\n",
    "# Process files\n",
    "output_lines = []\n",
    "for text_file in os.listdir(text_path):\n",
    "    text_file_path = os.path.join(text_path, text_file)\n",
    "    annotation_file_path = os.path.join(original_path, text_file.replace(\".txt\", \".ann\"))\n",
    "    \n",
    "    if os.path.exists(annotation_file_path):\n",
    "        # Read text\n",
    "        with open(text_file_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Parse annotations and create labels\n",
    "        entities = parse_annotations(annotation_file_path)\n",
    "        labels = create_iob_labels(text, entities)\n",
    "        token_labels = tokenize_and_label(text, labels)\n",
    "        \n",
    "        # Write to output\n",
    "        for token, label in token_labels:\n",
    "            output_lines.append(f\"{token}\\t{label}\")\n",
    "            if token.endswith(\".\"):  # Add a blank line after sentences\n",
    "                output_lines.append(\"\\n\")\n",
    "\n",
    "# Write the output to train.txt\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of special characters to remove\n",
    "special_characters = [\".\", \",\"]\n",
    "\n",
    "# Cleaning process\n",
    "with open(\"cadecv2/train.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = []\n",
    "for line in lines:\n",
    "    # Remove special characters\n",
    "    for char in special_characters:\n",
    "        line = line.replace(char, \"\")\n",
    "    cleaned_lines.append(line)\n",
    "\n",
    "# Writing the cleaned data to a new file\n",
    "with open(\"train2.txt\", \"w\") as file:\n",
    "    file.writelines(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the file\n",
    "with open(\"train2.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "processed_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:  # Skip empty lines\n",
    "        processed_lines.append(\"\\n\")\n",
    "        continue\n",
    "\n",
    "    if \"\\t\" in line:  # Process only lines with a tab (word-label pairs)\n",
    "        word, label = line.split(\"\\t\")\n",
    "        if \"'\" in word:  # Check if the word contains an apostrophe\n",
    "            base, suffix = word.split(\"'\", 1)  # Split the word at the apostrophe\n",
    "            processed_lines.append(f\"{base}\\t{label}\\n\")  # Add the base part\n",
    "            processed_lines.append(f\"'{suffix}\\t{label}\\n\")  # Add the suffix with the same label\n",
    "        else:\n",
    "            processed_lines.append(line + \"\\n\")  # Add the original line\n",
    "    else:\n",
    "        processed_lines.append(line + \"\\n\")  # Add lines without tabs as is\n",
    "\n",
    "# Writing the processed data to a new file\n",
    "with open(\"train3.txt\", \"w\") as file:\n",
    "    file.writelines(processed_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      "O: 85867\n",
      "B-ADR: 6469\n",
      "I-ADR: 8702\n",
      "B-Drug: 1761\n",
      "B-Disease: 288\n",
      "B-Symptom: 285\n",
      "I-Symptom: 266\n",
      "I-Disease: 171\n",
      "I-Drug: 176\n",
      "B-Finding: 450\n",
      "I-Finding: 392\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define the input file path\n",
    "input_file_path = \"train3.txt\"  # Replace with your file path\n",
    "\n",
    "# Initialize a counter for labels\n",
    "label_counts = Counter()\n",
    "\n",
    "# Processing the file to count labels\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if \"\\t\" in line:  # Process only lines with a tab (word-label pairs)\n",
    "            _, label = line.split(\"\\t\")\n",
    "            label_counts[label] += 1\n",
    "\n",
    "# Display the counts for each label\n",
    "print(\"Label counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"train3.txt\"  # Replace with your file path\n",
    "train_file_path = \"trainFinal.txt\"\n",
    "test_file_path = \"test.txt\"\n",
    "validation_file_path = \"validation.txt\"\n",
    "\n",
    "# Percentages for splitting\n",
    "test_split = 0.10\n",
    "validation_split = 0.10\n",
    "\n",
    "# Read the input file\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Separate sentences by blank lines\n",
    "sentences = []\n",
    "current_sentence = []\n",
    "for line in lines:\n",
    "    if line.strip():  # If the line is not empty\n",
    "        current_sentence.append(line)\n",
    "    else:  # If a blank line is encountered, store the sentence\n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "\n",
    "# Add the last sentence if file doesn't end with a blank line\n",
    "if current_sentence:\n",
    "    sentences.append(current_sentence)\n",
    "\n",
    "# Shuffle sentences\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Split sentences into train, test, and validation sets\n",
    "test_size = int(len(sentences) * test_split)\n",
    "validation_size = int(len(sentences) * validation_split)\n",
    "\n",
    "test_sentences = sentences[:test_size]\n",
    "validation_sentences = sentences[test_size:test_size + validation_size]\n",
    "train_sentences = sentences[test_size + validation_size:]\n",
    "\n",
    "# Function to write sentences to a file\n",
    "def write_sentences_to_file(sentences, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        for sentence in sentences:\n",
    "            for line in sentence:\n",
    "                file.write(line)\n",
    "            file.write(\"\\n\")  # Add a blank line between sentences\n",
    "\n",
    "# Write the splits to respective files\n",
    "write_sentences_to_file(train_sentences, train_file_path)\n",
    "write_sentences_to_file(test_sentences, test_file_path)\n",
    "write_sentences_to_file(validation_sentences, validation_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 5858/5858 [00:00<00:00, 127491.21 examples/s]\n",
      "Casting the dataset: 100%|██████████| 730/730 [00:00<00:00, 188583.51 examples/s]\n",
      "Casting the dataset: 100%|██████████| 733/733 [00:00<00:00, 43399.56 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 5858\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 730\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 733\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, Sequence, ClassLabel\n",
    "\n",
    "# Define the new labels\n",
    "label_names = [\n",
    "    \"O\", \"B-ADR\", \"I-ADR\", \"B-Drug\", \"I-Drug\",\n",
    "    \"B-Disease\", \"I-Disease\", \"B-Symptom\", \"I-Symptom\",\n",
    "    \"B-Finding\", \"I-Finding\"\n",
    "]\n",
    "\n",
    "# Create a mapping from label to integer ID\n",
    "label_mapping = {label: idx for idx, label in enumerate(label_names)}\n",
    "\n",
    "# Function to map labels to integers\n",
    "def label_to_id(label):\n",
    "    return label_mapping.get(label, -100)  # Return -100 for unknown labels\n",
    "\n",
    "# Function to read the BIO file\n",
    "def read_bio_file(filepath):\n",
    "    sentences = []\n",
    "    current_sentence = {\"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            orgline = line\n",
    "            line = line.strip()\n",
    "            if line == \"\":  # Sentence boundary\n",
    "                if current_sentence[\"tokens\"]:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = {\"tokens\": [], \"ner_tags\": []}\n",
    "            else:\n",
    "                # Split the line into token and label\n",
    "                parts = line.split()\n",
    "                if len(parts) == 2:  # Only process lines with exactly two parts\n",
    "                    token, label = parts\n",
    "                    current_sentence[\"tokens\"].append(token)\n",
    "                    current_sentence[\"ner_tags\"].append(label_to_id(label))\n",
    "                else:\n",
    "                    # TODO check how to add these\n",
    "                    #print(f\"Skipping malformed line: {orgline}\")\n",
    "                    continue\n",
    "\n",
    "        # Add the last sentence if the file doesn't end with a blank line\n",
    "        if current_sentence[\"tokens\"]:\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# Read the training dataset\n",
    "train_data = read_bio_file(\"trainFinal.txt\")\n",
    "val_data = read_bio_file(\"validation.txt\")\n",
    "test_data = read_bio_file(\"test.txt\")\n",
    "\n",
    "# Load data into the HuggingFace dataset structure\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict({\n",
    "        \"tokens\": [d[\"tokens\"] for d in train_data],\n",
    "        \"ner_tags\": [d[\"ner_tags\"] for d in train_data]\n",
    "    }),\n",
    "    \"test\": Dataset.from_dict({\n",
    "        \"tokens\": [d[\"tokens\"] for d in test_data],\n",
    "        \"ner_tags\": [d[\"ner_tags\"] for d in test_data]\n",
    "    }),\n",
    "    \"validation\": Dataset.from_dict({\n",
    "        \"tokens\": [d[\"tokens\"] for d in val_data],\n",
    "        \"ner_tags\": [d[\"ner_tags\"] for d in val_data]\n",
    "    })\n",
    "})\n",
    "\n",
    "# Define the ClassLabel feature for NER tags\n",
    "ner_feature = ClassLabel(names=label_names)\n",
    "\n",
    "# Cast the ner_tags column to use the ClassLabel feature\n",
    "dataset = dataset.cast_column(\"ner_tags\", Sequence(ner_feature))\n",
    "\n",
    "# Display the dataset structure\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5858/5858 [00:01<00:00, 4751.53 examples/s]\n",
      "Map: 100%|██████████| 730/730 [00:00<00:00, 6464.57 examples/s]\n",
      "Map: 100%|██████████| 733/733 [00:00<00:00, 4611.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Code obtained from: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "\n",
    "# Pre-processing the data and tokenize\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model is finetuned with the default hyperparameters settings on the train set and evaluate the model on the test set. These are the <b>baseline</b> results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code obtained from: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "\n",
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "tf_test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\emmav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "num_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\emmav\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "367/367 [==============================] - 1209s 3s/step - loss: 0.4391 - val_loss: 0.2886\n",
      "Epoch 2/3\n",
      " 44/367 [==>...........................] - ETA: 16:49 - loss: 0.2602"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_eval_dataset,\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tot hier gebleven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate on the test set\n",
    "for batch in tf_test_dataset:\n",
    "    logits = model.predict_on_batch(batch)[\"logits\"]\n",
    "    batch_labels = batch[\"labels\"].numpy()  # Avoid overwriting variable 'labels'\n",
    "    batch_predictions = np.argmax(logits, axis=-1)  # Avoid overwriting variable 'predictions'\n",
    "\n",
    "    for pred, true_label in zip(batch_predictions, batch_labels):\n",
    "        pred_sequence = []\n",
    "        label_sequence = []\n",
    "        for predicted_idx, label_idx in zip(pred, true_label):\n",
    "            if label_idx == -100:  # Skip padding\n",
    "                continue\n",
    "            pred_label = label_names[predicted_idx]\n",
    "            true_label_str = label_names[label_idx]\n",
    "            pred_sequence.append(pred_label)\n",
    "            label_sequence.append(true_label_str)\n",
    "        all_predictions.append(pred_sequence)\n",
    "        all_labels.append(label_sequence)\n",
    "\n",
    "# Compute overall metrics\n",
    "results = metric.compute(predictions=all_predictions, references=all_labels)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"Precision: {results['overall_precision']:.4f}\")\n",
    "print(f\"Recall: {results['overall_recall']:.4f}\")\n",
    "print(f\"F1-Score: {results['overall_f1']:.4f}\")\n",
    "print(f\"Accuracy: {results['overall_accuracy']:.4f}\")\n",
    "\n",
    "# Use seqeval's classification_report to get detailed per-entity metrics\n",
    "print(\"\\nPer-Entity Metrics:\")\n",
    "print(seqeval_classification_report(all_labels, all_predictions))\n",
    "\n",
    "# Calculate metrics for each label individually\n",
    "print(\"\\nMetrics Per Label:\")\n",
    "label_metrics = {}\n",
    "for label in ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ART', 'I-ART', 'I-MAT', 'B-MAT', 'I-CON', 'B-CON', 'I-SPE', 'B-SPE']:\n",
    "    # Filter predictions and references for the current label\n",
    "    label_predictions = [\n",
    "        [tag if tag == label else \"O\" for tag in pred_seq]\n",
    "        for pred_seq in all_predictions\n",
    "    ]\n",
    "    label_references = [\n",
    "        [tag if tag == label else \"O\" for tag in true_seq]\n",
    "        for true_seq in all_labels\n",
    "    ]\n",
    "\n",
    "    # Compute precision, recall, and F1 score for the specific label\n",
    "    label_result = metric.compute(predictions=label_predictions, references=label_references)\n",
    "    label_metrics[label] = {\n",
    "        \"precision\": label_result['overall_precision'],\n",
    "        \"recall\": label_result['overall_recall'],\n",
    "        \"f1\": label_result['overall_f1']\n",
    "    }\n",
    "\n",
    "# Print metrics for each label\n",
    "for label, metrics in label_metrics.items():\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rates\": [1e-5, 2e-5, 3e-5],\n",
    "    \"batch_sizes\": [8, 16, 32],\n",
    "    \"weight_decays\": [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "def random_search(params, results):\n",
    "    results = pd.DataFrame(columns=[\"learning_rate\", \"batch_size\", \"weight_decay\", \"val_loss\"])\n",
    "    # Perform a random search on 10 combinations\n",
    "    for i in range(10):\n",
    "        lr = random.choice(params[\"learning_rates\"])\n",
    "        batch_size = random.choice(params[\"batch_sizes\"])\n",
    "        weight_decay = random.choice(params[\"weight_decays\"])\n",
    "        print(f\"Training with learning rate: {lr}, Batch size: {batch_size}, and weight decay: {weight_decay}\")\n",
    "\n",
    "        # Create tf datasets with the current batch size\n",
    "        tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        # Set up optimizer with the current learning rate and weight decay\n",
    "        optimizer, schedule = create_optimizer(\n",
    "            init_lr=lr,\n",
    "            num_warmup_steps=0,\n",
    "            num_train_steps=len(tf_train_dataset) * num_epochs,\n",
    "            weight_decay_rate=weight_decay,\n",
    "        )\n",
    "        model.compile(optimizer=optimizer)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            tf_train_dataset,\n",
    "            validation_data=tf_eval_dataset,\n",
    "            epochs=num_epochs,\n",
    "        )\n",
    "\n",
    "        # Get the validation loss for the final epoch\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        # Append the results as a new DataFrame and concatenate\n",
    "        new_row = pd.DataFrame({\n",
    "            \"learning_rate\": [lr],\n",
    "            \"batch_size\": [batch_size],\n",
    "            \"weight_decay\": [weight_decay],\n",
    "            \"val_loss\": [val_loss]\n",
    "        })\n",
    "        results = pd.concat([results, new_row], ignore_index=True)\n",
    "    return results\n",
    "\n",
    "results = pd.DataFrame(columns=[\"learning_rate\", \"batch_size\", \"weight_decay\", \"val_loss\"])\n",
    "results = random_search(params, results)\n",
    "\n",
    "# Display all results\n",
    "print(results)\n",
    "results.to_csv(\"Results_question_4.csv\")\n",
    "\n",
    "# After optimization, evaluate on test set with best hyperparameters\n",
    "best_params = results.loc[results['val_loss'].idxmin()]\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train with best hyperparameters and evaluate on test set\n",
    "best_lr = float(best_params[\"learning_rate\"])\n",
    "best_batch_size = int(best_params[\"batch_size\"])\n",
    "best_weight_decay = float(best_params[\"weight_decay\"])\n",
    "\n",
    "print(best_lr, best_batch_size, best_weight_decay)\n",
    "\n",
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=best_batch_size,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=best_batch_size,\n",
    ")\n",
    "\n",
    "# Set up optimizer with the current learning rate and weight decay\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=best_lr,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=len(tf_train_dataset) * num_epochs,\n",
    "    weight_decay_rate=best_weight_decay,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_eval_dataset,\n",
    "    epochs=num_epochs,\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "for batch in tf_test_dataset:# Load seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate on the test set\n",
    "for batch in tf_test_dataset:\n",
    "    logits = model.predict_on_batch(batch)[\"logits\"]\n",
    "    batch_labels = batch[\"labels\"].numpy()  # Avoid overwriting variable 'labels'\n",
    "    batch_predictions = np.argmax(logits, axis=-1)  # Avoid overwriting variable 'predictions'\n",
    "\n",
    "    for pred, true_label in zip(batch_predictions, batch_labels):\n",
    "        pred_sequence = []\n",
    "        label_sequence = []\n",
    "        for predicted_idx, label_idx in zip(pred, true_label):\n",
    "            if label_idx == -100:  # Skip padding\n",
    "                continue\n",
    "            pred_label = label_names[predicted_idx]\n",
    "            true_label_str = label_names[label_idx]\n",
    "            pred_sequence.append(pred_label)\n",
    "            label_sequence.append(true_label_str)\n",
    "        all_predictions.append(pred_sequence)\n",
    "        all_labels.append(label_sequence)\n",
    "\n",
    "# Compute overall metrics\n",
    "results = metric.compute(predictions=all_predictions, references=all_labels)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"Precision: {results['overall_precision']:.4f}\")\n",
    "print(f\"Recall: {results['overall_recall']:.4f}\")\n",
    "print(f\"F1-Score: {results['overall_f1']:.4f}\")\n",
    "print(f\"Accuracy: {results['overall_accuracy']:.4f}\")\n",
    "\n",
    "# Use seqeval's classification_report to get detailed per-entity metrics\n",
    "print(\"\\nPer-Entity Metrics:\")\n",
    "print(seqeval_classification_report(all_labels, all_predictions))\n",
    "\n",
    "# Calculate metrics for each label individually\n",
    "print(\"\\nMetrics Per Label:\")\n",
    "label_metrics = {}\n",
    "for label in ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ART', 'I-ART', 'I-MAT', 'B-MAT', 'I-CON', 'B-CON', 'I-SPE', 'B-SPE']:\n",
    "    # Filter predictions and references for the current label\n",
    "    label_predictions = [\n",
    "        [tag if tag == label else \"O\" for tag in pred_seq]\n",
    "        for pred_seq in all_predictions\n",
    "    ]\n",
    "    label_references = [\n",
    "        [tag if tag == label else \"O\" for tag in true_seq]\n",
    "        for true_seq in all_labels\n",
    "    ]\n",
    "\n",
    "    # Compute precision, recall, and F1 score for the specific label\n",
    "    label_result = metric.compute(predictions=label_predictions, references=label_references)\n",
    "    label_metrics[label] = {\n",
    "        \"precision\": label_result['overall_precision'],\n",
    "        \"recall\": label_result['overall_recall'],\n",
    "        \"f1\": label_result['overall_f1']\n",
    "    }\n",
    "\n",
    "# Print metrics for each label\n",
    "for label, metrics in label_metrics.items():\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {metrics['f1']:.4f}\")\n",
    "    logits = model.predict_on_batch(batch)[\"logits\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for predicted_idx, label_idx in zip(prediction, label):\n",
    "            if label_idx == -100:\n",
    "                continue\n",
    "            all_predictions.append(label_names[predicted_idx])\n",
    "            all_labels.append(label_names[label_idx])\n",
    "\n",
    "test_metric = metric.compute(predictions=[all_predictions], references=[all_labels])\n",
    "print(\"Test set performance:\", test_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count label occurrences in the training dataset\n",
    "label_counter = Counter()\n",
    "for batch in tf_train_dataset:\n",
    "    labels = batch[\"labels\"].numpy()\n",
    "    for label_seq in labels:\n",
    "        for label in label_seq:\n",
    "            if label != -100:  # Exclude padding labels\n",
    "                label_counter[label_names[label]] += 1\n",
    "\n",
    "print(\"Label distribution in the training set:\")\n",
    "for label, count in label_counter.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
