{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TT1\\t300888008 ', ' Swelling of body region ', ' 55 59;60 68\\tbody swelling']\n",
      "['TT8\\t278528006 ', ' Facial swelling ', ' 60 68;70 74\\tswelling face']\n",
      "['TT9\\t298941006 ', ' Swelling of wrist joint ', ' 60 68;76 82\\tswelling wrists']\n",
      "['TT10\\t60728008 ', ' Abdominal swelling ', ' 60 68;84 91\\tswelling abdomen']\n",
      "['TT11\\t449614009 ', ' Swelling of lower limb ', ' 60 68;93 99\\tswelling thighs']\n",
      "['TT5\\t193462001 ', ' Insomnia ', ' 121 129\\tInsomina']\n",
      "['TT2\\t55533009 ', ' Forgetful ', ' + 40917007', 'Confusion', ' 152 179\\tforgetfulnes and confussion']\n",
      "['TT6\\t225014007 ', ' Feeling empty ', ' 223 246\\t\"empty stomach\" feeling']\n",
      "['TT3\\t398032003 ', ' Loose stool ', ' 282 294\\tloose stools']\n",
      "['TT7\\tCONCEPT_LESS 365 385\\tHA (Hyaluronic Acid)']\n",
      "['TT4\\t3384011000036100 ', ' Arthrotec ', ' 397 406\\tArthrotec']\n",
      "[[282, 294, 'ADR', '10024840', ' Loose stool '], [84, 91, 'ADR', '10042679'], [60, 68, 'ADR', '10042674', '10016065', '10042707', '10042679', '10011301', ' Facial swelling ', ' Swelling of wrist joint ', ' Abdominal swelling ', ' Swelling of lower limb '], [55, 59, 'ADR', '10042674', ' Swelling of body region '], [223, 246, 'ADR', 'CONCEPT_LESS', ' Feeling empty '], [76, 82, 'ADR', '10042707'], [397, 406, 'Drug', ' Arthrotec '], [152, 179, 'ADR', '10017060', ' Forgetful '], [93, 99, 'ADR', '10011301'], [365, 385, 'Drug'], [70, 74, 'ADR', '10016065'], [121, 129, 'ADR', '10022437', ' Insomnia ']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_annotations(file1_path, file2_path, file3_path):\n",
    "    entities = []\n",
    "\n",
    "    # Parse the first file\n",
    "    with open(file1_path, \"r\") as f1:\n",
    "        for line in f1:\n",
    "            if line.startswith(\"T\"):  # Entity line\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                tag_number, entity_info, text = parts\n",
    "                entity_type, *offsets = entity_info.split(\" \")\n",
    "\n",
    "                # Handle multiple offset ranges\n",
    "                offset_ranges = \" \".join(offsets).split(\";\")\n",
    "                for offset_range in offset_ranges:\n",
    "                    try:\n",
    "                        start_offset, end_offset = map(int, offset_range.split(\" \"))\n",
    "                        entities.append([start_offset, end_offset, entity_type])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "    # Parse the second file and add mappings\n",
    "    with open(file2_path, \"r\") as f2:\n",
    "        for line in f2:\n",
    "            if line.startswith(\"TT\"):\n",
    "                parts = re.split(r'[\\t;\\+\" \"]', line)\n",
    "                for entry in parts:\n",
    "                    if entry.isdigit():\n",
    "                        entry = int(entry)\n",
    "                        for count, tag in enumerate(entities):\n",
    "                            if entry == tag[0]:\n",
    "                                entities[count].append(parts[1])\n",
    "\n",
    "    # Parse the third file and add entries\n",
    "    with open(file3_path, \"r\") as f3:\n",
    "        for line in f3:\n",
    "            if line.startswith(\"TT\"):\n",
    "                parts = line.strip().split(\"|\")  # Split by tab\n",
    "                print(parts)\n",
    "                offset = parts[-1].split(\"\\t\")[-2].split(\" \")\n",
    "                \n",
    "                if len(parts) > 1:\n",
    "                    for entry in offset:\n",
    "                        if entry.isdigit():\n",
    "\n",
    "                            entry = int(entry)\n",
    "                            for count, tag in enumerate(entities):\n",
    "                                if entry == tag[0]:\n",
    "                                    entities[count].append(parts[1])\n",
    "                            break\n",
    "    entities = list(map(list, set(map(tuple, entities))))\n",
    "    return entities\n",
    "\n",
    "print(parse_annotations(\"cadecv2/original/ARTHROTEC.105.ann\", \n",
    "                        \"cadecv2/meddra/ARTHROTEC.105.ann\", \n",
    "                        \"cadecv2/sct/ARTHROTEC.105.ann\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OutputFolder\\\\trainAll.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_annotations(file1_path, file2_path, file3_path):\n",
    "    entities = []\n",
    "    meddra_entities = []\n",
    "    sct_entities = []\n",
    "\n",
    "    # Parse the first file\n",
    "    with open(file1_path, \"r\") as f1:\n",
    "        for line in f1:\n",
    "            if line.startswith(\"T\"):  # Entity line\n",
    "                parts = line.strip().split(\"\\t\")\n",
    "                tag_number, entity_info, text = parts\n",
    "                entity_type, *offsets = entity_info.split(\" \")\n",
    "                offset_ranges = \" \".join(offsets).split(\";\")\n",
    "                for offset_range in offset_ranges:\n",
    "                    try:\n",
    "                        start_offset, end_offset = map(int, offset_range.split(\" \"))\n",
    "                        entities.append([start_offset, end_offset, entity_type])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "    # Parse the second file for MedDRA mappings\n",
    "    if os.path.exists(file2_path):\n",
    "        with open(file2_path, \"r\") as f2:\n",
    "            for line in f2:\n",
    "                if line.startswith(\"TT\"):\n",
    "                    parts = re.split(r'[\\t;\\+\\\" \"]', line)\n",
    "                    for entry in parts:\n",
    "                        if entry.isdigit():\n",
    "                            entry = int(entry)\n",
    "                            for tag in entities:\n",
    "                                if entry == tag[0]:\n",
    "                                    meddra_entities.append(tag + [parts[1]])\n",
    "\n",
    "    # Parse the third file for SCT mappings\n",
    "    if os.path.exists(file3_path):\n",
    "        with open(file3_path, \"r\") as f3:\n",
    "            for line in f3:\n",
    "                if line.startswith(\"TT\"):\n",
    "                    parts = line.strip().split(\"|\")\n",
    "                    try:\n",
    "                        offset = parts[-1].split(\"\\t\")[-2].split(\" \")\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if len(parts) > 1:\n",
    "                        for entry in offset:\n",
    "                            if entry.isdigit():\n",
    "                                entry = int(entry)\n",
    "                                for tag in entities:\n",
    "                                    if entry == tag[0]:\n",
    "                                        sct_entities.append(tag + [parts[1]])\n",
    "\n",
    "    entities = list(map(list, set(map(tuple, entities))))\n",
    "    meddra_entities = list(map(list, set(map(tuple, meddra_entities))))\n",
    "    sct_entities = list(map(list, set(map(tuple, sct_entities))))\n",
    "\n",
    "    return entities, meddra_entities, sct_entities\n",
    "\n",
    "def create_iob_labels(text, entities):\n",
    "    labels = [\"O\"] * len(text)\n",
    "    label_details = [None] * len(text)\n",
    "\n",
    "    for start, end, entity_type, *potential_labels in entities:\n",
    "        primary_label = entity_type\n",
    "        potential_ids = potential_labels if potential_labels else []\n",
    "        labels[start] = f\"B-{primary_label}\"\n",
    "        label_details[start] = potential_ids\n",
    "\n",
    "        for i in range(start + 1, end):\n",
    "            labels[i] = f\"I-{primary_label}\"\n",
    "            label_details[i] = potential_ids\n",
    "\n",
    "    return labels, label_details\n",
    "\n",
    "def tokenize_and_label(text, labels, label_details):\n",
    "    token_labels = []\n",
    "    text_index = 0\n",
    "\n",
    "    while text_index < len(text):\n",
    "        if text[text_index].isspace():\n",
    "            text_index += 1\n",
    "            continue\n",
    "\n",
    "        if re.match(r\"\\W\", text[text_index]):\n",
    "            label = labels[text_index]\n",
    "            token_detail = label_details[text_index]\n",
    "            detail = f\"\\t{' '.join(token_detail)}\" if token_detail else \"\"\n",
    "            token_labels.append((text[text_index], label, detail))\n",
    "            text_index += 1\n",
    "        else:\n",
    "            end_index = text_index\n",
    "            while end_index < len(text) and re.match(r\"\\w\", text[end_index]):\n",
    "                end_index += 1\n",
    "\n",
    "            token = text[text_index:end_index]\n",
    "            token_label = labels[text_index:end_index]\n",
    "            label = token_label[0] if token_label else \"O\"\n",
    "            token_detail = label_details[text_index]\n",
    "            detail = f\"\\t{' '.join(token_detail)}\" if token_detail else \"\"\n",
    "            token_labels.append((token, label, detail))\n",
    "            text_index = end_index\n",
    "\n",
    "    return token_labels\n",
    "\n",
    "def write_to_file(output_path, token_labels):\n",
    "    with open(output_path, \"w\") as f:\n",
    "        for token, label, detail in token_labels:\n",
    "            # Write formatted token, label, and detail\n",
    "            detail_str = f\"\\t{detail}\" if detail else \"\"\n",
    "            f.write(f\"{token}\\t{label}{detail_str}\\n\")\n",
    "            if token == \".\":\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "# Define paths\n",
    "base_path = \"cadecv2\"\n",
    "original_path = os.path.join(base_path, \"original\")\n",
    "text_path = os.path.join(base_path, \"text\")\n",
    "meddra_path = os.path.join(base_path, \"meddra\")\n",
    "sct_path = os.path.join(base_path, \"sct\")\n",
    "output_org_path = os.path.join(\"OutputFolder\", \"trainOrg.txt\")\n",
    "output_meddra_path = os.path.join(\"OutputFolder\", \"trainMeddra.txt\")\n",
    "output_sct_path = os.path.join(\"OutputFolder\", \"trainSct.txt\")\n",
    "output_all_labels_path = os.path.join(\"OutputFolder\", \"trainAll.txt\")\n",
    "\n",
    "output_org_lines = []\n",
    "output_meddra_lines = []\n",
    "output_sct_lines = []\n",
    "output_all_labels_lines = []\n",
    "\n",
    "for text_file in os.listdir(text_path):\n",
    "    text_file_path = os.path.join(text_path, text_file)\n",
    "    annotation_file_path = os.path.join(original_path, text_file.replace(\".txt\", \".ann\"))\n",
    "    meddra_mapping_path = os.path.join(meddra_path, text_file.replace(\".txt\", \".ann\"))\n",
    "    sct_mapping_path = os.path.join(sct_path, text_file.replace(\".txt\", \".ann\"))\n",
    "\n",
    "    if os.path.exists(annotation_file_path):\n",
    "        with open(text_file_path, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        entities, meddra_entities, sct_entities = parse_annotations(annotation_file_path, meddra_mapping_path, sct_mapping_path)\n",
    "\n",
    "        # Process Org labels\n",
    "        labels, label_details = create_iob_labels(text, entities)\n",
    "        token_labels_original = tokenize_and_label(text, labels, label_details)\n",
    "        output_org_lines.extend(token_labels_original)\n",
    "\n",
    "        # Process MedDRA labels\n",
    "        labels, label_details = create_iob_labels(text, meddra_entities)\n",
    "        token_labels_meddra = tokenize_and_label(text, labels, label_details)\n",
    "        output_meddra_lines.extend(token_labels_meddra)\n",
    "\n",
    "        # Process SCT labels\n",
    "        labels, label_details = create_iob_labels(text, sct_entities)\n",
    "        token_labels_sct = tokenize_and_label(text, labels, label_details)\n",
    "        output_sct_lines.extend(token_labels_sct)\n",
    "\n",
    "# Write trainOrg.txt\n",
    "write_to_file(output_org_path, output_org_lines)\n",
    "write_to_file(output_meddra_path, output_meddra_lines)\n",
    "write_to_file(output_sct_path, output_sct_lines)\n",
    "\n",
    "# Make trainAll.txt identical to trainMeddra.txt\n",
    "import shutil\n",
    "shutil.copy(output_meddra_path, output_all_labels_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the source and target files\n",
    "with open('OutputFolder/trainSct.txt', 'r') as source_file, open('OutputFolder/trainAll.txt', 'r+') as target_file:\n",
    "    # Read all lines from trainAll.txt into a list\n",
    "    all_lines = target_file.readlines()\n",
    "    \n",
    "    # Loop through each line in the source file\n",
    "    for count, line in enumerate(source_file):\n",
    "        # Remove trailing whitespace and split the line into entries\n",
    "        entries = line.strip().split()\n",
    "        labels = []\n",
    "        \n",
    "        # Collect labels starting from the 3rd entry\n",
    "        for entry in entries[2:]:\n",
    "            labels.append(entry)\n",
    "        \n",
    "        # Ensure we're within bounds for trainAll.txt\n",
    "        if count < len(all_lines):\n",
    "            # Add a tab and then the labels to the corresponding line in trainAll.txt\n",
    "            all_lines[count] = all_lines[count].strip() + '\\t' + ' '.join(labels) + '\\n'\n",
    "        else:\n",
    "            print(f\"Warning: Line {count + 1} in trainMeddra.txt exceeds trainAll.txt lines.\")\n",
    "    \n",
    "    # Move to the beginning of the file and write the updated lines\n",
    "    target_file.seek(0)\n",
    "    target_file.writelines(all_lines)\n",
    "    target_file.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      "O: 102670\n",
      "B-ADR: 6777\n",
      "I-ADR: 9143\n",
      "B-Drug: 1802\n",
      "B-Disease: 285\n",
      "B-Symptom: 289\n",
      "I-Symptom: 277\n",
      "I-Disease: 206\n",
      "I-Drug: 237\n",
      "B-Finding: 452\n",
      "I-Finding: 414\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define the input file path\n",
    "input_file_path = \"OutputFolder/trainOrg.txt\"  # Replace with your file path\n",
    "\n",
    "# Initialize a counter for labels\n",
    "label_counts = Counter()\n",
    "\n",
    "# Processing the file to count labels\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if \"\\t\" in line:  # Process only lines with a tab (word-label pairs)\n",
    "            _, label = line.split(\"\\t\")\n",
    "            label_counts[label] += 1\n",
    "\n",
    "# Display the counts for each label\n",
    "print(\"Label counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference file sentences: 7520\n",
      "Target file sentences: 7520\n",
      "Aligned file saved to OutputFolder/aligned_trainAll_normalized.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Normalize file by standardizing line endings and cleaning whitespace\n",
    "def normalize_file(input_path, output_path, encoding='ISO-8859-1'):\n",
    "    \"\"\"\n",
    "    Normalize a text file by standardizing line endings and removing extra whitespace.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding=encoding) as infile:\n",
    "        content = infile.read()\n",
    "    \n",
    "    # Normalize line endings and remove trailing/leading spaces\n",
    "    content = content.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    content = re.sub(r'[ \\t]+$', '', content, flags=re.MULTILINE)  # Remove trailing spaces\n",
    "    content = re.sub(r'\\n{3,}', '\\n\\n', content)  # Replace multiple empty lines with a double newline\n",
    "    content = content.strip()\n",
    "    \n",
    "    with open(output_path, 'w', encoding=encoding) as outfile:\n",
    "        outfile.write(content + '\\n')\n",
    "\n",
    "# Normalize both files\n",
    "reference_normalized = 'OutputFolder/trainOrg_normalized.txt'\n",
    "target_normalized = 'OutputFolder/trainAll_normalized.txt'\n",
    "\n",
    "normalize_file('Outputfolder/trainOrg.txt', reference_normalized)\n",
    "normalize_file('OutputFolder/trainAll.txt', target_normalized)\n",
    "\n",
    "# Check sentence counts again\n",
    "def count_sentences(file_path, encoding='ISO-8859-1'):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        content = file.read()\n",
    "    return len(content.strip().split('\\n\\n'))\n",
    "\n",
    "org_sentence_count = count_sentences(reference_normalized)\n",
    "all_sentence_count = count_sentences(target_normalized)\n",
    "\n",
    "print(f\"Reference file sentences: {org_sentence_count}\")\n",
    "print(f\"Target file sentences: {all_sentence_count}\")\n",
    "\n",
    "# Align normalized files\n",
    "def align_sentences_token_by_token(reference_path, target_path, output_path, encoding='ISO-8859-1'):\n",
    "    \"\"\"\n",
    "    Align sentences at a token level to ensure both line and sentence consistency.\n",
    "    \"\"\"\n",
    "    with open(reference_path, 'r', encoding=encoding) as ref_file, open(target_path, 'r', encoding=encoding) as tgt_file:\n",
    "        ref_sentences = ref_file.read().strip().split('\\n\\n')\n",
    "        tgt_sentences = tgt_file.read().strip().split('\\n\\n')\n",
    "\n",
    "    if len(ref_sentences) != len(tgt_sentences):\n",
    "        raise ValueError(\"Sentence counts still do not match after normalization. Please inspect the files.\")\n",
    "\n",
    "    aligned_sentences = []\n",
    "\n",
    "    for i, (ref_sentence, tgt_sentence) in enumerate(zip(ref_sentences, tgt_sentences)):\n",
    "        ref_tokens = [line.strip() for line in ref_sentence.split('\\n') if line.strip()]\n",
    "        tgt_tokens = [line.strip() for line in tgt_sentence.split('\\n') if line.strip()]\n",
    "\n",
    "        if len(ref_tokens) != len(tgt_tokens):\n",
    "            print(f\"❗ Token count mismatch in sentence {i+1}: Expected {len(ref_tokens)} tokens, got {len(tgt_tokens)} tokens\")\n",
    "            # Fallback alignment\n",
    "            aligned_sentence = '\\n'.join(tgt_tokens[:len(ref_tokens)])\n",
    "        else:\n",
    "            aligned_sentence = '\\n'.join(tgt_tokens)\n",
    "        \n",
    "        aligned_sentences.append(aligned_sentence)\n",
    "\n",
    "    with open(output_path, 'w', encoding=encoding) as out_file:\n",
    "        out_file.write('\\n\\n'.join(aligned_sentences) + '\\n')\n",
    "\n",
    "    print(f\"Aligned file saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Final Alignment\n",
    "output_file = 'OutputFolder/aligned_trainAll_normalized.txt'\n",
    "align_sentences_token_by_token(reference_normalized, target_normalized, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Org: 7520 sentences\n",
      "Meddra: 7520 sentences\n",
      "Sct: 7520 sentences\n",
      "All: 7520 sentences\n",
      "✅ Train/Test/Validation split completed successfully! Files saved in 'Train-test-split'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "output_folder = 'Train-test-split'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define file paths\n",
    "files = {\n",
    "    \"Org\": \"OutputFolder/trainOrg.txt\",\n",
    "    \"Meddra\": \"OutputFolder/trainMeddra.txt\",\n",
    "    \"Sct\": \"OutputFolder/trainSct.txt\",\n",
    "    \"All\": \"OutputFolder/aligned_trainAll_normalized.txt\"\n",
    "}\n",
    "\n",
    "# Define output files in Train-test-split folder\n",
    "output_files = {\n",
    "    \"train\": {key: os.path.join(output_folder, f\"train_{key}.txt\") for key in files},\n",
    "    \"test\": os.path.join(output_folder, \"test_Org.txt\"),\n",
    "    \"validation\": os.path.join(output_folder, \"validation_Org.txt\")\n",
    "}\n",
    "\n",
    "# Helper function to read sentences from a file\n",
    "def read_sentences(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        content = file.read().strip()\n",
    "    # Normalize line endings and split sentences by double newlines\n",
    "    content = content.replace('\\r\\n', '\\n').strip()\n",
    "    sentences = [sentence.strip() for sentence in content.split('\\n\\n') if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "# Count sentences in all files\n",
    "for key, path in files.items():\n",
    "    sentences = read_sentences(path)\n",
    "    print(f\"{key}: {len(sentences)} sentences\")\n",
    "\n",
    "# Helper function to write sentences to a file\n",
    "def write_sentences(filepath, sentences):\n",
    "    with open(filepath, 'w') as file:\n",
    "        file.write('\\n\\n'.join(sentences) + '\\n')\n",
    "\n",
    "# Step 1: Read sentences from all files\n",
    "sentences = {key: read_sentences(files[key]) for key in files}\n",
    "\n",
    "# Check consistency\n",
    "sentence_count = len(sentences['Org'])\n",
    "assert all(len(sentences[key]) == sentence_count for key in sentences), \"Sentence counts are inconsistent across files!\"\n",
    "\n",
    "# Step 2: Randomly split indices into train, test, and validation\n",
    "indices = list(range(sentence_count))\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_ratio, test_ratio, val_ratio = 0.8, 0.1, 0.1\n",
    "train_end = int(train_ratio * sentence_count)\n",
    "test_end = train_end + int(test_ratio * sentence_count)\n",
    "\n",
    "train_idx = indices[:train_end]\n",
    "test_idx = indices[train_end:test_end]\n",
    "val_idx = indices[test_end:]\n",
    "\n",
    "# Step 3: Write to output files\n",
    "\n",
    "# Write train splits (same sentences for all four train files)\n",
    "for key in files:\n",
    "    train_sentences = [sentences[key][i] for i in train_idx]\n",
    "    write_sentences(output_files['train'][key], train_sentences)\n",
    "\n",
    "# Write test and validation splits (only for Org)\n",
    "test_sentences = [sentences['Org'][i] for i in test_idx]\n",
    "validation_sentences = [sentences['Org'][i] for i in val_idx]\n",
    "\n",
    "write_sentences(output_files['test'], test_sentences)\n",
    "write_sentences(output_files['validation'], validation_sentences)\n",
    "\n",
    "print(f\"✅ Train/Test/Validation split completed successfully! Files saved in '{output_folder}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
