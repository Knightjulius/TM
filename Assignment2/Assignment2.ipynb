{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ztJoT_2HWcJ"
   },
   "source": [
    "# Text mining assignment 2 (Emma Vonk and Julius Ruijgrok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "l-M5asKdHWcT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report as seqeval_classification_report\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_-P4OwivyYL"
   },
   "source": [
    "# Question 2\n",
    "Convert the IOB data to the correct data structure for token classification in Huggingface\n",
    "(words and labels like the conll2023 data in the tutorial) and align the labels with the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hDMtXFXHHWcU"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dd788d28c74211a011e203e30e83c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1992 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5aa586611e34801af78c53ded36083b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d163a93d75649ee99ff076572b3a881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to make the dataset to the correct huggingface structure explained in: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "\n",
    "\n",
    "def read_bio_file(filepath):\n",
    "    sentences = []\n",
    "    current_sentence = {\"tokens\": [], \"ner_tags\": [], \"pos_tags\": []}\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if current_sentence[\"tokens\"]:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = {\"tokens\": [], \"ner_tags\": [], \"pos_tags\": []}\n",
    "            else:\n",
    "                token, pos, label = line.split()  # Each line is a token POS-label\n",
    "                current_sentence[\"tokens\"].append(token)\n",
    "                # Map labels to an integer ID\n",
    "                current_sentence[\"ner_tags\"].append(label_to_id(label))\n",
    "                current_sentence[\"pos_tags\"].append(pos)\n",
    "\n",
    "        # Add the last sentence if file doesn't end with a blank line\n",
    "        if current_sentence[\"tokens\"]:\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def label_to_id(label):\n",
    "    # This function should map each label to a unique integer (e.g., B-PER -> 0, I-PER -> 1, O -> 2).\n",
    "    label_mapping = {\"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-LOC\": 3, \"I-LOC\": 4, 'B-ART': 5, 'I-ART': 6, 'I-MAT': 7, 'B-MAT': 8, 'I-CON': 9, 'B-CON': 10, 'I-SPE': 11, 'B-SPE': 12}\n",
    "    return label_mapping.get(label, -100)  # Return -100 for unknown labels\n",
    "\n",
    "# Read datasets\n",
    "train_data = read_bio_file(\"train.txt\")\n",
    "val_data = read_bio_file(\"val.txt\")\n",
    "test_data = read_bio_file(\"test.txt\")\n",
    "\n",
    "# Load into HuggingFace dataset structure\n",
    "dataset = datasets.DatasetDict({\n",
    "    \"train\": datasets.Dataset.from_dict({\"tokens\": [d[\"tokens\"] for d in train_data], \"ner_tags\": [d[\"ner_tags\"] for d in train_data], \"pos_tags\": [d[\"pos_tags\"] for d in train_data]}),\n",
    "    \"validation\": datasets.Dataset.from_dict({\"tokens\": [d[\"tokens\"] for d in val_data], \"ner_tags\": [d[\"ner_tags\"] for d in val_data], \"pos_tags\": [d[\"pos_tags\"] for d in val_data]}),\n",
    "    \"test\": datasets.Dataset.from_dict({\"tokens\": [d[\"tokens\"] for d in test_data], \"ner_tags\": [d[\"ner_tags\"] for d in test_data], \"pos_tags\": [d[\"pos_tags\"] for d in test_data\n",
    "                                                                                                                                                ]})\n",
    "})\n",
    "\n",
    "# Define label mapping\n",
    "label_names = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ART', 'I-ART', 'I-MAT', 'B-MAT', 'I-CON', 'B-CON', 'I-SPE', 'B-SPE']\n",
    "\n",
    "# Create the ClassLabel feature with only the names (otherwise the number does not overlap)\n",
    "ner_feature = datasets.ClassLabel(names=label_names)\n",
    "\n",
    "dataset = dataset.cast_column(\"ner_tags\", datasets.Sequence(ner_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Gua40EeHHWcW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'pos_tags'],\n",
       "        num_rows: 1992\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'pos_tags'],\n",
       "        num_rows: 850\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'pos_tags'],\n",
       "        num_rows: 864\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the structure of the data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "aXGdQMinHWcY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758a4b91cda04999abbdff0f8b9bff26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1992 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583ea531744d4039a6de834f4477200c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be24bf7c09244e485634ef631d4c3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/864 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code obtained from: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "\n",
    "# Pre-processing the data and tokenize\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk39Kgihyfzb"
   },
   "source": [
    "Now, the pre-processing of the data is finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awMovOGeHWcZ"
   },
   "source": [
    "# Question 3\n",
    "Fine-tune a model with the default hyperparameter settings on the train set and evaluate the model on the test set. These are your baseline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QWedt7uYHWca"
   },
   "outputs": [],
   "source": [
    "# Code obtained from: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(\n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "f9s63bOIHWca"
   },
   "outputs": [],
   "source": [
    "# Code obtained from: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "\n",
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "tf_test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "m8i_UL6FHWcb"
   },
   "outputs": [],
   "source": [
    "# Code obtained from: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "D5BX-_tPHWcb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Code obtained from: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    }
   ],
   "source": [
    "# Code obtained from: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "num_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset) * num_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "125/125 [==============================] - 1170s 9s/step - loss: 0.4644 - val_loss: 0.2759\n",
      "Epoch 2/3\n",
      "125/125 [==============================] - 1069s 9s/step - loss: 0.1556 - val_loss: 0.2556\n",
      "Epoch 3/3\n",
      "125/125 [==============================] - 1069s 9s/step - loss: 0.1109 - val_loss: 0.2506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e2a74c2890>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code obtained from: https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n",
    "model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_eval_dataset,\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Metrics:\n",
      "Precision: 0.4983\n",
      "Recall: 0.6293\n",
      "F1-Score: 0.5562\n",
      "Accuracy: 0.9535\n",
      "\n",
      "Per-Entity Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ART       0.35      0.59      0.44       168\n",
      "         CON       0.38      0.58      0.46       216\n",
      "         LOC       0.54      0.71      0.61       144\n",
      "         MAT       0.00      0.00      0.00       107\n",
      "         PER       0.72      0.89      0.80       283\n",
      "         SPE       0.00      0.00      0.00         2\n",
      "\n",
      "   micro avg       0.50      0.63      0.56       920\n",
      "   macro avg       0.33      0.46      0.38       920\n",
      "weighted avg       0.46      0.63      0.53       920\n",
      "\n",
      "\n",
      "Metrics Per Label:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: O\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n",
      "Label: B-PER\n",
      "  Precision: 0.7681\n",
      "  Recall: 0.9364\n",
      "  F1-Score: 0.8439\n",
      "Label: I-PER\n",
      "  Precision: 0.5455\n",
      "  Recall: 0.7156\n",
      "  F1-Score: 0.6190\n",
      "Label: B-LOC\n",
      "  Precision: 0.7202\n",
      "  Recall: 0.8403\n",
      "  F1-Score: 0.7756\n",
      "Label: I-LOC\n",
      "  Precision: 0.5290\n",
      "  Recall: 0.6518\n",
      "  F1-Score: 0.5840\n",
      "Label: B-ART\n",
      "  Precision: 0.4901\n",
      "  Recall: 0.7381\n",
      "  F1-Score: 0.5891\n",
      "Label: I-ART\n",
      "  Precision: 0.2832\n",
      "  Recall: 0.4667\n",
      "  F1-Score: 0.3525\n",
      "Label: I-MAT\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n",
      "Label: B-MAT\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n",
      "Label: I-CON\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n",
      "Label: B-CON\n",
      "  Precision: 0.4464\n",
      "  Recall: 0.6944\n",
      "  F1-Score: 0.5435\n",
      "Label: I-SPE\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n",
      "Label: B-SPE\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Load seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate on the test set\n",
    "for batch in tf_test_dataset:\n",
    "    logits = model.predict_on_batch(batch)[\"logits\"]\n",
    "    batch_labels = batch[\"labels\"].numpy()  # Avoid overwriting variable 'labels'\n",
    "    batch_predictions = np.argmax(logits, axis=-1)  # Avoid overwriting variable 'predictions'\n",
    "\n",
    "    for pred, true_label in zip(batch_predictions, batch_labels):\n",
    "        pred_sequence = []\n",
    "        label_sequence = []\n",
    "        for predicted_idx, label_idx in zip(pred, true_label):\n",
    "            if label_idx == -100:  # Skip padding\n",
    "                continue\n",
    "            pred_label = label_names[predicted_idx]\n",
    "            true_label_str = label_names[label_idx]\n",
    "            pred_sequence.append(pred_label)\n",
    "            label_sequence.append(true_label_str)\n",
    "        all_predictions.append(pred_sequence)\n",
    "        all_labels.append(label_sequence)\n",
    "\n",
    "# Compute overall metrics\n",
    "results = metric.compute(predictions=all_predictions, references=all_labels)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"Precision: {results['overall_precision']:.4f}\")\n",
    "print(f\"Recall: {results['overall_recall']:.4f}\")\n",
    "print(f\"F1-Score: {results['overall_f1']:.4f}\")\n",
    "print(f\"Accuracy: {results['overall_accuracy']:.4f}\")\n",
    "\n",
    "# Use seqeval's classification_report to get detailed per-entity metrics\n",
    "print(\"\\nPer-Entity Metrics:\")\n",
    "print(seqeval_classification_report(all_labels, all_predictions))\n",
    "\n",
    "# Calculate metrics for each label individually\n",
    "print(\"\\nMetrics Per Label:\")\n",
    "label_metrics = {}\n",
    "for label in ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ART', 'I-ART', 'I-MAT', 'B-MAT', 'I-CON', 'B-CON', 'I-SPE', 'B-SPE']:\n",
    "    # Filter predictions and references for the current label\n",
    "    label_predictions = [\n",
    "        [tag if tag == label else \"O\" for tag in pred_seq]\n",
    "        for pred_seq in all_predictions\n",
    "    ]\n",
    "    label_references = [\n",
    "        [tag if tag == label else \"O\" for tag in true_seq]\n",
    "        for true_seq in all_labels\n",
    "    ]\n",
    "\n",
    "    # Compute precision, recall, and F1 score for the specific label\n",
    "    label_result = metric.compute(predictions=label_predictions, references=label_references)\n",
    "    label_metrics[label] = {\n",
    "        \"precision\": label_result['overall_precision'],\n",
    "        \"recall\": label_result['overall_recall'],\n",
    "        \"f1\": label_result['overall_f1']\n",
    "    }\n",
    "\n",
    "# Print metrics for each label\n",
    "for label, metrics in label_metrics.items():\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2VpKNPsHWcc"
   },
   "source": [
    "# Question 4\n",
    "Set up hyperparameter optimization (HPO), use the val set as validation. Optimize at least three hyperparameters (learning rate, batch size and weight decay). You can choose your own way to implement this and select your own grid. After the model has been optimized, evaluate the result on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "125/125 [==============================] - 1098s 9s/step - loss: 0.5080 - val_loss: 0.3464\n",
      "Epoch 2/3\n",
      "125/125 [==============================] - 1035s 8s/step - loss: 0.2218 - val_loss: 0.2752\n",
      "Epoch 3/3\n",
      "125/125 [==============================] - 995s 8s/step - loss: 0.1667 - val_loss: 0.2655\n",
      "Test {n} out of 6 has concluded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juliu\\AppData\\Local\\Temp\\ipykernel_17820\\2365670061.py:59: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "125/125 [==============================] - 1027s 8s/step - loss: 0.1379 - val_loss: 0.2417\n",
      "Epoch 2/3\n",
      "125/125 [==============================] - 995s 8s/step - loss: 0.0987 - val_loss: 0.2437\n",
      "Epoch 3/3\n",
      "125/125 [==============================] - 1000s 8s/step - loss: 0.0840 - val_loss: 0.2493\n",
      "Test {n} out of 6 has concluded\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 1191s 19s/step - loss: 0.0811 - val_loss: 0.2399\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 1148s 18s/step - loss: 0.0653 - val_loss: 0.2400\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 1160s 19s/step - loss: 0.0581 - val_loss: 0.2470\n",
      "Test {n} out of 6 has concluded\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 1209s 19s/step - loss: 0.0577 - val_loss: 0.2569\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 1195s 19s/step - loss: 0.0471 - val_loss: 0.2679\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 1222s 19s/step - loss: 0.0408 - val_loss: 0.2720\n",
      "Test {n} out of 6 has concluded\n",
      "Epoch 1/3\n",
      "125/125 [==============================] - 1052s 8s/step - loss: 0.0506 - val_loss: 0.2807\n",
      "Epoch 2/3\n",
      "125/125 [==============================] - 1039s 8s/step - loss: 0.0342 - val_loss: 0.3031\n",
      "Epoch 3/3\n",
      "125/125 [==============================] - 1049s 8s/step - loss: 0.0232 - val_loss: 0.3082\n",
      "Test {n} out of 6 has concluded\n",
      "Epoch 1/3\n",
      "125/125 [==============================] - 1060s 8s/step - loss: 0.0305 - val_loss: 0.3060\n",
      "Epoch 2/3\n",
      "125/125 [==============================] - 1029s 8s/step - loss: 0.0192 - val_loss: 0.3257\n",
      "Epoch 3/3\n",
      "125/125 [==============================] - 1036s 8s/step - loss: 0.0131 - val_loss: 0.3357\n",
      "Test {n} out of 6 has concluded\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 1249s 19s/step - loss: 0.0161 - val_loss: 0.3118\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 1203s 19s/step - loss: 0.0114 - val_loss: 0.3305\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 1218s 19s/step - loss: 0.0085 - val_loss: 0.3438\n",
      "Test {n} out of 6 has concluded\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 1226s 19s/step - loss: 0.0114 - val_loss: 0.3693\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 1226s 18s/step - loss: 0.0074 - val_loss: 0.3704\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 1208s 19s/step - loss: 0.0058 - val_loss: 0.3692\n",
      "Test {n} out of 6 has concluded\n",
      "   learning_rate batch_size  weight_decay  val_loss\n",
      "0        0.00001         16         0.010  0.265483\n",
      "1        0.00001         16         0.001  0.249348\n",
      "2        0.00001         32         0.010  0.246995\n",
      "3        0.00001         32         0.001  0.272050\n",
      "4        0.00002         16         0.010  0.308213\n",
      "5        0.00002         16         0.001  0.335678\n",
      "6        0.00002         32         0.010  0.343800\n",
      "7        0.00002         32         0.001  0.369150\n",
      "Best hyperparameters: learning_rate     0.00001\n",
      "batch_size             32\n",
      "weight_decay         0.01\n",
      "val_loss         0.246995\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"learning_rates\": [1e-5, 2e-5, 3e-5],\n",
    "    \"batch_sizes\": [8, 16, 32],\n",
    "    \"weight_decays\": [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "def random_search(params, results):\n",
    "    results = pd.DataFrame(columns=[\"learning_rate\", \"batch_size\", \"weight_decay\", \"val_loss\"])\n",
    "    # Perform a random search on 10 combinations\n",
    "    for i in range(10):\n",
    "        lr = random.choice(params[\"learning_rates\"])\n",
    "        batch_size = random.choice(params[\"batch_sizes\"])\n",
    "        weight_decay = random.choice(params[\"weight_decays\"])\n",
    "        print(f\"Training with learning rate: {lr}, Batch size: {batch_size}, and weight decay: {weight_decay}\")\n",
    "\n",
    "        # Create tf datasets with the current batch size\n",
    "        tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "            columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        # Set up optimizer with the current learning rate and weight decay\n",
    "        optimizer, schedule = create_optimizer(\n",
    "            init_lr=lr,\n",
    "            num_warmup_steps=0,\n",
    "            num_train_steps=len(tf_train_dataset) * num_epochs,\n",
    "            weight_decay_rate=weight_decay,\n",
    "        )\n",
    "        model.compile(optimizer=optimizer)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            tf_train_dataset,\n",
    "            validation_data=tf_eval_dataset,\n",
    "            epochs=num_epochs,\n",
    "        )\n",
    "\n",
    "        # Get the validation loss for the final epoch\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        # Append the results as a new DataFrame and concatenate\n",
    "        new_row = pd.DataFrame({\n",
    "            \"learning_rate\": [lr],\n",
    "            \"batch_size\": [batch_size],\n",
    "            \"weight_decay\": [weight_decay],\n",
    "            \"val_loss\": [val_loss]\n",
    "        })\n",
    "        results = pd.concat([results, new_row], ignore_index=True)\n",
    "    return results\n",
    "\n",
    "results = pd.DataFrame(columns=[\"learning_rate\", \"batch_size\", \"weight_decay\", \"val_loss\"])\n",
    "results = random_search(params, results)\n",
    "\n",
    "# Display all results\n",
    "print(results)\n",
    "results.to_csv(\"Results_question_4.csv\")\n",
    "\n",
    "# After optimization, evaluate on test set with best hyperparameters\n",
    "best_params = results.loc[results['val_loss'].idxmin()]\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-05 32 0.01\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 1276s 20s/step - loss: 0.0994 - val_loss: 0.2405\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 1178s 19s/step - loss: 0.0794 - val_loss: 0.2411\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 1149s 18s/step - loss: 0.0702 - val_loss: 0.2458\n",
      "Test set performance: {'ART': {'precision': 0.375, 'recall': 0.6071428571428571, 'f1': 0.46363636363636357, 'number': 168}, 'CON': {'precision': 0.39263803680981596, 'recall': 0.5925925925925926, 'f1': 0.47232472324723246, 'number': 216}, 'LOC': {'precision': 0.5614973262032086, 'recall': 0.7291666666666666, 'f1': 0.6344410876132931, 'number': 144}, 'MAT': {'precision': 0.5, 'recall': 0.04672897196261682, 'f1': 0.08547008547008547, 'number': 107}, 'PER': {'precision': 0.7175792507204611, 'recall': 0.8798586572438163, 'f1': 0.7904761904761906, 'number': 283}, 'SPE': {'precision': 0.1111111111111111, 'recall': 0.5, 'f1': 0.1818181818181818, 'number': 2}, 'overall_precision': 0.5125977410947002, 'overall_recall': 0.6413043478260869, 'overall_f1': 0.5697730564944471, 'overall_accuracy': 0.9551446356645917}\n"
     ]
    }
   ],
   "source": [
    "# Re-train with best hyperparameters and evaluate on test set\n",
    "best_lr = float(best_params[\"learning_rate\"])\n",
    "best_batch_size = int(best_params[\"batch_size\"])\n",
    "best_weight_decay = float(best_params[\"weight_decay\"])\n",
    "\n",
    "print(best_lr, best_batch_size, best_weight_decay)\n",
    "\n",
    "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=best_batch_size,\n",
    ")\n",
    "tf_eval_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=best_batch_size,\n",
    ")\n",
    "\n",
    "# Set up optimizer with the current learning rate and weight decay\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=best_lr,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=len(tf_train_dataset) * num_epochs,\n",
    "    weight_decay_rate=best_weight_decay,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_eval_dataset,\n",
    "    epochs=num_epochs,\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "for batch in tf_test_dataset:\n",
    "    logits = model.predict_on_batch(batch)[\"logits\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for predicted_idx, label_idx in zip(prediction, label):\n",
    "            if label_idx == -100:\n",
    "                continue\n",
    "            all_predictions.append(label_names[predicted_idx])\n",
    "            all_labels.append(label_names[label_idx])\n",
    "\n",
    "test_metric = metric.compute(predictions=[all_predictions], references=[all_labels])\n",
    "print(\"Test set performance:\", test_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Extend the evaluation function so that it shows the Precision, Recall and F-score for each of the entity types (location, artefact, etc.) on the test set. Include the metrics for the B-label of the entity type, the I-label, and the full entities.\n",
    "\n",
    "# Question 6\n",
    "Look up the definitions of macro- and micro-average scores and compute the macro- and micro average F1 scores over all entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1yyWYG6HWcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Metrics:\n",
      "Precision: 0.5126\n",
      "Recall: 0.6413\n",
      "F1-Score: 0.5698\n",
      "Accuracy: 0.9551\n",
      "\n",
      "Per-Entity Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ART       0.38      0.61      0.46       168\n",
      "         CON       0.39      0.59      0.47       216\n",
      "         LOC       0.56      0.73      0.63       144\n",
      "         MAT       0.50      0.05      0.09       107\n",
      "         PER       0.72      0.88      0.79       283\n",
      "         SPE       0.11      0.50      0.18         2\n",
      "\n",
      "   micro avg       0.51      0.64      0.57       920\n",
      "   macro avg       0.44      0.56      0.44       920\n",
      "weighted avg       0.53      0.64      0.55       920\n",
      "\n",
      "\n",
      "Metrics Per Label:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\Juliu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: O\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n",
      "Label: B-PER\n",
      "  Precision: 0.7729\n",
      "  Recall: 0.9258\n",
      "  F1-Score: 0.8424\n",
      "Label: I-PER\n",
      "  Precision: 0.5166\n",
      "  Recall: 0.7156\n",
      "  F1-Score: 0.6000\n",
      "Label: B-LOC\n",
      "  Precision: 0.7326\n",
      "  Recall: 0.8750\n",
      "  F1-Score: 0.7975\n",
      "Label: I-LOC\n",
      "  Precision: 0.5814\n",
      "  Recall: 0.6696\n",
      "  F1-Score: 0.6224\n",
      "Label: B-ART\n",
      "  Precision: 0.5191\n",
      "  Recall: 0.7262\n",
      "  F1-Score: 0.6055\n",
      "Label: I-ART\n",
      "  Precision: 0.2994\n",
      "  Recall: 0.4762\n",
      "  F1-Score: 0.3676\n",
      "Label: I-MAT\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n",
      "Label: B-MAT\n",
      "  Precision: 0.5000\n",
      "  Recall: 0.0467\n",
      "  F1-Score: 0.0855\n",
      "Label: I-CON\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n",
      "Label: B-CON\n",
      "  Precision: 0.4663\n",
      "  Recall: 0.7037\n",
      "  F1-Score: 0.5609\n",
      "Label: I-SPE\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1-Score: 0.0000\n",
      "Label: B-SPE\n",
      "  Precision: 0.1111\n",
      "  Recall: 0.5000\n",
      "  F1-Score: 0.1818\n"
     ]
    }
   ],
   "source": [
    "# Load seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Evaluate on the test set\n",
    "for batch in tf_test_dataset:\n",
    "    logits = model.predict_on_batch(batch)[\"logits\"]\n",
    "    batch_labels = batch[\"labels\"].numpy()  # Avoid overwriting variable 'labels'\n",
    "    batch_predictions = np.argmax(logits, axis=-1)  # Avoid overwriting variable 'predictions'\n",
    "\n",
    "    for pred, true_label in zip(batch_predictions, batch_labels):\n",
    "        pred_sequence = []\n",
    "        label_sequence = []\n",
    "        for predicted_idx, label_idx in zip(pred, true_label):\n",
    "            if label_idx == -100:  # Skip padding\n",
    "                continue\n",
    "            pred_label = label_names[predicted_idx]\n",
    "            true_label_str = label_names[label_idx]\n",
    "            pred_sequence.append(pred_label)\n",
    "            label_sequence.append(true_label_str)\n",
    "        all_predictions.append(pred_sequence)\n",
    "        all_labels.append(label_sequence)\n",
    "\n",
    "# Compute overall metrics\n",
    "results = metric.compute(predictions=all_predictions, references=all_labels)\n",
    "\n",
    "print(\"Overall Metrics:\")\n",
    "print(f\"Precision: {results['overall_precision']:.4f}\")\n",
    "print(f\"Recall: {results['overall_recall']:.4f}\")\n",
    "print(f\"F1-Score: {results['overall_f1']:.4f}\")\n",
    "print(f\"Accuracy: {results['overall_accuracy']:.4f}\")\n",
    "\n",
    "# Use seqeval's classification_report to get detailed per-entity metrics\n",
    "print(\"\\nPer-Entity Metrics:\")\n",
    "print(seqeval_classification_report(all_labels, all_predictions))\n",
    "\n",
    "# Calculate metrics for each label individually\n",
    "print(\"\\nMetrics Per Label:\")\n",
    "label_metrics = {}\n",
    "for label in ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ART', 'I-ART', 'I-MAT', 'B-MAT', 'I-CON', 'B-CON', 'I-SPE', 'B-SPE']:\n",
    "    # Filter predictions and references for the current label\n",
    "    label_predictions = [\n",
    "        [tag if tag == label else \"O\" for tag in pred_seq]\n",
    "        for pred_seq in all_predictions\n",
    "    ]\n",
    "    label_references = [\n",
    "        [tag if tag == label else \"O\" for tag in true_seq]\n",
    "        for true_seq in all_labels\n",
    "    ]\n",
    "\n",
    "    # Compute precision, recall, and F1 score for the specific label\n",
    "    label_result = metric.compute(predictions=label_predictions, references=label_references)\n",
    "    label_metrics[label] = {\n",
    "        \"precision\": label_result['overall_precision'],\n",
    "        \"recall\": label_result['overall_recall'],\n",
    "        \"f1\": label_result['overall_f1']\n",
    "    }\n",
    "\n",
    "# Print metrics for each label\n",
    "for label, metrics in label_metrics.items():\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in the training set:\n",
      "O: 57424\n",
      "B-ART: 1000\n",
      "I-ART: 965\n",
      "B-PER: 1213\n",
      "I-PER: 1631\n",
      "B-CON: 1423\n",
      "B-MAT: 185\n",
      "I-MAT: 26\n",
      "I-CON: 153\n",
      "B-LOC: 342\n",
      "I-LOC: 679\n",
      "B-SPE: 184\n",
      "I-SPE: 9\n"
     ]
    }
   ],
   "source": [
    "# Count label occurrences in the training dataset\n",
    "label_counter = Counter()\n",
    "for batch in tf_train_dataset:\n",
    "    labels = batch[\"labels\"].numpy()\n",
    "    for label_seq in labels:\n",
    "        for label in label_seq:\n",
    "            if label != -100:  # Exclude padding labels\n",
    "                label_counter[label_names[label]] += 1\n",
    "\n",
    "print(\"Label distribution in the training set:\")\n",
    "for label, count in label_counter.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
